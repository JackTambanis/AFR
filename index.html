<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
  <title>Pan — Multi-face AI Expression Detector (Single File)</title>
  <meta name="description" content="Real-time multi-face detection + expression labeling in the browser — no backend, works on iPad (Safari) and mobile browsers. Uses face-api.js models from a public CDN." />
  <style>
    :root{
      --bg: #0f1724; --card:#0b1220; --accent:#7c3aed; --muted:#94a3b8; --white:#eef2ff;
    }
    html,body{height:100%;margin:0;background:linear-gradient(180deg,var(--bg),#071024);color:var(--white);font-family:Inter,ui-sans-serif,system-ui,-apple-system,'Segoe UI',Roboto,'Helvetica Neue',Arial}
    .app{display:grid;grid-template-columns:1fr;grid-auto-rows:min-content;gap:12px;max-width:1100px;margin:18px auto;padding:12px}
    header{display:flex;gap:12px;align-items:center}
    h1{font-size:18px;margin:0}
    .controls{display:flex;gap:8px;margin-left:auto;align-items:center}
    button,select{background:linear-gradient(180deg,#0e1726,#051028);border:1px solid rgba(255,255,255,0.04);color:var(--white);padding:8px 10px;border-radius:10px;font-size:14px}
    button.secondary{background:transparent;border:1px solid rgba(255,255,255,0.06);color:var(--muted)}
    .stage{position:relative;border-radius:12px;overflow:hidden;background:rgba(255,255,255,0.02);box-shadow:0 6px 30px rgba(2,6,23,0.6)}
    video#inputVideo{width:100%;height:auto;display:block;object-fit:cover;transform:scaleX(-1)}
    canvas#overlay{position:absolute;inset:0;pointer-events:none;transform:scaleX(-1)}
    .status{padding:10px;color:var(--muted);font-size:13px}
    .tips{font-size:13px;color:var(--muted)}
    .small{font-size:12px;color:var(--muted)}
    footer{display:flex;justify-content:space-between;align-items:center;padding:8px 0}
    .label-pill{background:rgba(0,0,0,0.6);padding:6px 8px;border-radius:999px;border:1px solid rgba(255,255,255,0.04);font-size:13px}
    .toggle{display:flex;gap:6px;align-items:center}
    .loader{width:14px;height:14px;border-radius:50%;border:2px solid rgba(255,255,255,0.08);border-top-color:var(--accent);animation:spin 1s linear infinite}
    @keyframes spin{to{transform:rotate(360deg)}}
    .controls-row{display:flex;gap:8px;align-items:center}
    .confidence{font-weight:600}
    @media(min-width:900px){.app{grid-template-columns:1fr 360px;align-items:start}.stage{grid-column:1/2}.sidebar{grid-column:2/3}}
    .sidebar{padding:12px;background:linear-gradient(180deg,rgba(255,255,255,0.02),transparent);border-radius:12px}
    .muted-block{background:rgba(255,255,255,0.02);padding:10px;border-radius:8px}
  </style>
</head>
<body>
  <div class="app">
    <header>
      <div>
        <h1>Pan — Multi-face AI Expression Detector</h1>
        <div class="small">Client-only, runs on iPad & mobile. Uses face-api.js (TinyFaceDetector + FaceLandmark + Expressions).</div>
      </div>
      <div class="controls">
        <div class="controls-row">
          <label class="small label-pill" id="modelStatus">Loading models...</label>
          <select id="facingSelect" title="Camera facing" aria-label="Camera facing">
            <option value="user">Front camera</option>
            <option value="environment">Back camera</option>
          </select>
          <button id="startBtn">Start</button>
          <button id="snapBtn" class="secondary">Snapshot</button>
        </div>
      </div>
    </header>

    <div class="stage">
      <video id="inputVideo" autoplay muted playsinline></video>
      <canvas id="overlay"></canvas>
    </div>

    <div class="sidebar">
      <div class="muted-block">
        <div class="status"><span id="fps">—</span> FPS • <span id="facesCount">0</span> faces detected</div>
        <div class="tips">Tip: for best results on iPad, allow camera access and switch to back camera if needed. Make sure faces are well-lit and mostly facing camera.</div>
      </div>
      <div style="height:12px"></div>
      <div class="muted-block">
        <div style="display:flex;justify-content:space-between;align-items:center"><strong>Detection settings</strong><span class="small">(performance)</span></div>
        <div style="height:8px"></div>
        <label class="small">Input size: <span id="inputSizeLabel">160</span></label>
        <input id="inputSizeRange" type="range" min="128" max="512" step="32" value="160" />
        <div style="height:10px"></div>
        <label class="small">Score threshold: <span id="scoreLabel">0.5</span></label>
        <input id="scoreRange" type="range" min="0.2" max="0.8" step="0.05" value="0.5" />
      </div>
      <div style="height:12px"></div>
      <div class="muted-block">
        <strong>Expressions legend</strong>
        <div class="small">happy • sad • angry • fearful • disgusted • surprised • neutral</div>
      </div>
      <footer>
        <div class="small">Models from public CDN — no upload. Works offline after models cached.</div>
        <div><button id="stopBtn" class="secondary">Stop</button></div>
      </footer>
    </div>
  </div>

<script src="https://unpkg.com/face-api.js/dist/face-api.min.js"></script>
<script>
  // Single-file pan face expression detector
  const MODEL_URL = 'https://justadudewhohacks.github.io/face-api.js/models';
  const video = document.getElementById('inputVideo');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');
  const modelStatus = document.getElementById('modelStatus');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const snapBtn = document.getElementById('snapBtn');
  const facingSelect = document.getElementById('facingSelect');
  const fpsLabel = document.getElementById('fps');
  const facesCount = document.getElementById('facesCount');
  const inputSizeRange = document.getElementById('inputSizeRange');
  const scoreRange = document.getElementById('scoreRange');
  const inputSizeLabel = document.getElementById('inputSizeLabel');
  const scoreLabel = document.getElementById('scoreLabel');

  let stream = null;
  let running = false;
  let lastTime = performance.now();
  let frameCount = 0;
  let fps = 0;

  // detection options
  function getTinyOptions(){
    const inputSize = parseInt(inputSizeRange.value);
    const scoreThreshold = parseFloat(scoreRange.value);
    return new faceapi.TinyFaceDetectorOptions({inputSize, scoreThreshold});
  }

  inputSizeRange.addEventListener('input', ()=>{inputSizeLabel.textContent = inputSizeRange.value});
  scoreRange.addEventListener('input', ()=>{scoreLabel.textContent = scoreRange.value});

  async function loadModels(){
    modelStatus.textContent = 'Loading models...';
    try{
      // tiny_face_detector, face_landmark_68, face_expression
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
      ]);
      modelStatus.innerHTML = '<span class="label-pill">Models loaded</span>';
    }catch(e){
      console.error('Model load error', e);
      modelStatus.textContent = 'Model load failed — check network or open via HTTPS.';
    }
  }

  async function startCamera(){
    if(running) return;
    const facingMode = facingSelect.value || 'user';
    const constraints = {audio:false, video:{facingMode, width:{ideal:1280}, height:{ideal:720}}};
    try{
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = stream;
      await video.play();
      running = true;
      canvas.width = video.videoWidth || video.clientWidth || 640;
      canvas.height = video.videoHeight || video.clientHeight || 480;
      runDetectionLoop();
    }catch(err){
      console.error('Camera error', err);
      alert('Unable to access camera. Make sure you are on HTTPS and allowed camera access.');
    }
  }

  function stopCamera(){
    running = false;
    if(stream){
      stream.getTracks().forEach(t=>t.stop());
      stream = null;
    }
    ctx.clearRect(0,0,canvas.width,canvas.height);
    facesCount.textContent = '0';
    fpsLabel.textContent = '—';
  }

  async function runDetectionLoop(){
    if(!running) return;
    const now = performance.now();
    frameCount++;
    if(now - lastTime >= 1000){
      fps = Math.round((frameCount*1000)/(now-lastTime));
      fpsLabel.textContent = fps;
      frameCount = 0; lastTime = now;
    }

    // ensure canvas size matches video size
    if(canvas.width !== video.videoWidth || canvas.height !== video.videoHeight){
      canvas.width = video.videoWidth || video.clientWidth;
      canvas.height = video.videoHeight || video.clientHeight;
    }

    const options = getTinyOptions();
    // detect all faces with expressions and landmarks
    const results = await faceapi.detectAllFaces(video, options).withFaceLandmarks().withFaceExpressions();

    // clear
    ctx.clearRect(0,0,canvas.width,canvas.height);

    // draw each detection
    facesCount.textContent = results.length;

    results.forEach((res, i)=>{
      const {detection, landmarks, expressions} = res;
      const box = detection.box;
      const x = box.x; const y = box.y; const w = box.width; const h = box.height;

      // box
      ctx.lineWidth = Math.max(2, Math.round(canvas.width/640));
      ctx.strokeStyle = 'rgba(124,58,237,0.9)';
      ctx.strokeRect(x, y, w, h);

      // label — best expression
      const expr = bestExpression(expressions);
      const conf = Math.round(expressions[expr]*100);
      const label = `${expr} (${conf}%)`;

      const padding = 6; const fontSize = Math.max(12, Math.round(canvas.width/480));
      ctx.font = `600 ${fontSize}px Inter, sans-serif`;
      ctx.textBaseline = 'top';
      const textW = ctx.measureText(label).width;
      const labelX = x;
      const labelY = Math.max(0, y - fontSize - padding);

      // background for text
      ctx.fillStyle = 'rgba(6,10,20,0.7)';
      roundRect(ctx, labelX - 2, labelY - 4, textW + padding*2, fontSize + padding, 6, true, false);

      // text
      ctx.fillStyle = '#fff';
      ctx.fillText(label, labelX + padding - 2, labelY + 2);

      // draw landmarks (optional small dots)
      const pts = landmarks.positions;
      ctx.fillStyle = 'rgba(124,58,237,0.9)';
      for(let p=0;p<pts.length;p+=4){
        const pt = pts[p];
        ctx.beginPath(); ctx.arc(pt.x, pt.y, Math.max(1, fontSize/6), 0, Math.PI*2); ctx.fill();
      }

    });

    // next frame
    requestAnimationFrame(runDetectionLoop);
  }

  function bestExpression(exp){
    let best='neutral'; let bestScore=0;
    for(const k in exp){ if(exp[k] > bestScore){bestScore = exp[k]; best = k;} }
    return best;
  }

  function roundRect(ctx, x, y, w, h, r, fill, stroke){
    if(typeof r === 'undefined') r=5; if(typeof r === 'number') r={tl:r,tr:r,br:r,bl:r};
    ctx.beginPath(); ctx.moveTo(x+r.tl, y); ctx.lineTo(x+w-r.tr, y); ctx.quadraticCurveTo(x+w, y, x+w, y+r.tr);
    ctx.lineTo(x+w, y+h-r.br); ctx.quadraticCurveTo(x+w, y+h, x+w-r.br, y+h);
    ctx.lineTo(x+r.bl, y+h); ctx.quadraticCurveTo(x, y+h, x, y+h-r.bl);
    ctx.lineTo(x, y+r.tl); ctx.quadraticCurveTo(x, y, x+r.tl, y); ctx.closePath();
    if(fill) ctx.fill(); if(stroke) ctx.stroke();
  }

  // snapshot
  snapBtn.addEventListener('click', ()=>{
    if(!video || video.readyState < 2) return alert('Video not ready');
    const w = canvas.width; const h = canvas.height;
    const tmp = document.createElement('canvas'); tmp.width = w; tmp.height = h; const tctx = tmp.getContext('2d');
    // copy video (mirrored) correctly by drawing flipped
    tctx.save(); tctx.scale(-1,1); tctx.drawImage(video, -w, 0, w, h); tctx.restore();
    // overlay (draw detection canvas as well)
    tctx.drawImage(canvas, 0, 0);
    const url = tmp.toDataURL('image/png');
    const a = document.createElement('a'); a.href = url; a.download = 'pan-snapshot.png'; a.click();
  });

  startBtn.addEventListener('click', async ()=>{
    if(!faceapi.nets.tinyFaceDetector.params) await loadModels();
    await startCamera();
  });
  stopBtn.addEventListener('click', ()=>stopCamera());

  // load models at startup but don't open camera until user presses start (important on iOS)
  loadModels();

  // handle page visibility / stopping
  document.addEventListener('visibilitychange', ()=>{ if(document.hidden) stopCamera(); });

  // helpers: handle orientation/resizing
  window.addEventListener('resize', ()=>{
    if(video && video.videoWidth){ canvas.width = video.videoWidth; canvas.height = video.videoHeight; }
  });

  // extra: allow switching camera while running
  facingSelect.addEventListener('change', async ()=>{
    if(running){ stopCamera(); await startCamera(); }
  });

  // friendly UX: warn user for iOS specifics
  (function iOSNotice(){
    const ua = navigator.userAgent || '';
    if(/iPhone|iPad|iPod/.test(ua)){
      const div = document.createElement('div'); div.className='small'; div.style.marginTop='8px'; div.style.color='var(--muted)';
      div.textContent = 'Note: iOS Safari requires user gesture to start camera and works best in Safari. Add to Home Screen for fullscreen.';
      document.querySelector('.sidebar').prepend(div);
    }
  })();

</script>
</body>
</html>
