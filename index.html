<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Face & Expression Wireframe + 3D Mesh</title>
<style>
  html, body { margin: 0; height: 100%; overflow: hidden; background: #000; }
  #container { display: flex; width: 100%; height: 100%; }
  #left, #right { position: relative; flex: 1; }
  #video { width: 100%; height: 100%; object-fit: cover; }
  canvas { position: absolute; top: 0; left: 0; }
</style>
</head>
<body>
<div id="container">
  <div id="left">
    <video id="video" autoplay muted></video>
    <canvas id="overlayLeft"></canvas>
  </div>
  <div id="right">
    <canvas id="overlayRight"></canvas>
  </div>
</div>

<!-- face-api.js for 2D wireframe -->
<script src="https://cdn.jsdelivr.net/npm/face-api.js/dist/face-api.min.js"></script>

<!-- Three.js for 3D rendering -->
<script src="https://cdn.jsdelivr.net/npm/three@0.154.0/build/three.min.js"></script>

<!-- MediaPipe FaceMesh -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/facemesh@0.4/facemesh.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

<script>
window.addEventListener('DOMContentLoaded', async () => {
  const video = document.getElementById('video');
  const overlayLeft = document.getElementById('overlayLeft');
  const overlayRight = document.getElementById('overlayRight');
  const ctxLeft = overlayLeft.getContext('2d');

  // Load face-api.js models
  await Promise.all([
    faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@master/weights'),
    faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@master/weights'),
    faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@master/weights')
  ]);

  // Start webcam
  const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
  video.srcObject = stream;

  // Setup Three.js scene for right canvas
  const scene = new THREE.Scene();
  const camera = new THREE.PerspectiveCamera(45, overlayRight.width / overlayRight.height, 0.1, 1000);
  const renderer = new THREE.WebGLRenderer({ canvas: overlayRight, alpha: true });
  renderer.setSize(overlayRight.clientWidth, overlayRight.clientHeight);

  const light = new THREE.DirectionalLight(0xffffff, 1);
  light.position.set(0, 0, 2);
  scene.add(light);

  const geometry = new THREE.BufferGeometry();
  const material = new THREE.PointsMaterial({ color: 0x00ffcc, size: 0.02 });
  const facePoints = new THREE.Points(geometry, material);
  scene.add(facePoints);

  camera.position.z = 2;

  function animateThree() {
    requestAnimationFrame(animateThree);
    renderer.render(scene, camera);
  }
  animateThree();

  // Setup MediaPipe FaceMesh
  const faceMesh = new FaceMesh.FaceMesh({
    locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/facemesh/${file}`
  });
  faceMesh.setOptions({
    maxNumFaces: 1,
    refineLandmarks: true,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });

  faceMesh.onResults(results => {
    if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) return;

    const landmarks = results.multiFaceLandmarks[0];

    // Update Three.js geometry with 3D landmarks
    const positions = new Float32Array(landmarks.length * 3);
    for (let i = 0; i < landmarks.length; i++) {
      positions[i*3] = (landmarks[i].x - 0.5) * 2;   // X
      positions[i*3+1] = -(landmarks[i].y - 0.5) * 2; // Y
      positions[i*3+2] = -landmarks[i].z * 2;        // Z
    }
    geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    geometry.computeBoundingSphere();
  });

  const cameraUtils = new CameraUtils.Camera(video, {
    onFrame: async () => {
      await faceMesh.send({ image: video });
    },
    width: 640,
    height: 480
  });
  cameraUtils.start();

  // Face-api.js 2D overlay drawing
  video.addEventListener('play', () => {
    const displaySize = { width: video.offsetWidth, height: video.offsetHeight };
    overlayLeft.width = displaySize.width;
    overlayLeft.height = displaySize.height;
    faceapi.matchDimensions(overlayLeft, displaySize);

    setInterval(async () => {
      const detections = await faceapi
        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions({ inputSize: 224 }))
        .withFaceLandmarks()
        .withFaceExpressions();

      const resized = faceapi.resizeResults(detections, displaySize);
      ctxLeft.clearRect(0,0,overlayLeft.width, overlayLeft.height);

      faceapi.draw.drawDetections(overlayLeft, resized);
      faceapi.draw.drawFaceExpressions(overlayLeft, resized);
      faceapi.draw.drawFaceLandmarks(overlayLeft, resized);
    }, 100);
  });
});
</script>
</body>
</html>
