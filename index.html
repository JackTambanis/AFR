<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Webcam: Face Expressions + Object Detection (No age/race/gender)</title>
  <style>
    :root { --bg:#0e0f12; --panel:#14161a; --line:#2a2f39; --text:#e7e9ee; --muted:#99a1ad; }
    *{box-sizing:border-box}
    html,body{height:100%;margin:0;background:linear-gradient(180deg,#0b0c10,#0e0f12 30%,#101218);color:var(--text);font:14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial}
    .wrap{max-width:1100px;margin:24px auto;padding:16px}
    h1{font-size:20px;margin:0 0 8px;letter-spacing:.3px}
    p{color:var(--muted);margin:6px 0 14px}
    .stage{position:relative;background:#07080c;border:1px solid var(--line);border-radius:14px;overflow:hidden;box-shadow:0 10px 40px rgba(0,0,0,.35)}
    video,canvas{display:block;width:100%;height:auto}
    #overlay{position:absolute;left:0;top:0;pointer-events:none}
    .hud{
      position:absolute;left:12px;top:12px;display:flex;gap:8px;flex-wrap:wrap;z-index:3
    }
    .chip{
      background:rgba(20,22,26,.75);backdrop-filter: blur(8px);
      border:1px solid var(--line);border-radius:999px;padding:6px 10px;color:var(--text);
      display:flex;align-items:center;gap:8px
    }
    .controls{display:flex;flex-wrap:wrap;gap:10px;margin:12px 0 0}
    button, label.toggle{
      background:var(--panel);border:1px solid var(--line);color:var(--text);
      border-radius:10px;padding:10px 14px;cursor:pointer
    }
    button:disabled{opacity:.5;cursor:not-allowed}
    .toggles{display:flex;gap:10px;align-items:center}
    .legend{margin-top:10px;color:var(--muted)}
    .r{border-left:3px solid #d9534f;padding-left:8px}
    .g{border-left:3px solid #5cb85c;padding-left:8px}
    .y{border-left:3px solid #f0ad4e;padding-left:8px}
    .note{font-size:12px;color:#9aa3af;margin-top:8px}
    .footer{margin-top:12px;color:#7f8793}
    .meter{height:4px;border-radius:2px;background:#1f232b;overflow:hidden}
    .meter > i{display:block;height:100%;background:#6aa3ff;width:0%}
  </style>
</head>
<body>
  <div class="wrap">
    <h1>Live Facial Expressions + Object Detection</h1>
    <p>
      Shows webcam video with on-screen labels. Facial analysis estimates expressions only
      (e.g., happy/neutral/surprised/angry)—<b>no age, race, or gender</b>.
    </p>

    <div class="controls">
      <button id="btnStart">Start Camera</button>
      <button id="btnStop" disabled>Stop</button>
      <div class="toggles">
        <label class="toggle"><input type="checkbox" id="toggleFaces" checked /> Faces</label>
        <label class="toggle"><input type="checkbox" id="toggleExpr" checked /> Expressions</label>
        <label class="toggle"><input type="checkbox" id="toggleObjs" checked /> Objects</label>
        <label class="toggle"><input type="checkbox" id="toggleMirror" checked /> Mirror</label>
      </div>
    </div>
    <div class="legend">
      <div class="g">Expressions are inferred from MediaPipe <i>blendshapes</i> (mouth/brow/eyes) locally on your device.</div>
      <div class="y">On iPhone/iPad, you must tap “Start Camera” and use HTTPS (e.g., GitHub Pages).</div>
    </div>

    <div class="stage" id="stage">
      <video id="webcam" playsinline muted></video>
      <canvas id="overlay"></canvas>
      <div class="hud">
        <div class="chip">FPS: <span id="fps">0</span></div>
        <div class="chip">Resolution: <span id="res">–</span></div>
        <div class="chip">Detections: <span id="det">0</span></div>
      </div>
    </div>

    <div class="note">Tip: If the camera appears black on iOS, ensure <b>Settings → Safari → Camera</b> is allowed and the page uses HTTPS.</div>
    <div class="footer">Powered by MediaPipe Tasks (Face Landmarker & Object Detector), running fully in-browser.</div>
  </div>

  <!-- App -->
  <script type="module">
    // Import MediaPipe Tasks Vision
    import {
      FaceLandmarker, ObjectDetector, FilesetResolver, DrawingUtils
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14";

    const els = {
      video: document.getElementById('webcam'),
      canvas: document.getElementById('overlay'),
      fps: document.getElementById('fps'),
      res: document.getElementById('res'),
      det: document.getElementById('det'),
      btnStart: document.getElementById('btnStart'),
      btnStop: document.getElementById('btnStop'),
      toggleFaces: document.getElementById('toggleFaces'),
      toggleExpr: document.getElementById('toggleExpr'),
      toggleObjs: document.getElementById('toggleObjs'),
      toggleMirror: document.getElementById('toggleMirror'),
      stage: document.getElementById('stage')
    };

    let stream = null;
    let faceLandmarker = null;
    let objectDetector = null;
    let running = false;
    let lastTime = performance.now();
    let fpsSMA = 0;
    let detectionsCount = 0;
    let objEveryNFrames = 2; // throttle object detector
    let frameId = 0;

    const MP_WASM_BASE = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm";
    // MediaPipe official hosted models:
    const FACE_LANDMARKER_TASK = "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task";
    const OBJECT_DETECTOR_MODEL = "https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/float16/1/efficientdet_lite0.tflite";

    function lerp(a,b,t){return a+(b-a)*t;}

    async function loadModels(){
      const fileset = await FilesetResolver.forVisionTasks(MP_WASM_BASE);
      faceLandmarker = await FaceLandmarker.createFromOptions(fileset, {
        baseOptions: { modelAssetPath: FACE_LANDMARKER_TASK },
        runningMode: "VIDEO",
        numFaces: 4,
        outputFaceBlendshapes: true,
        outputFacialTransformationMatrixes: false
      });
      objectDetector = await ObjectDetector.createFromOptions(fileset, {
        baseOptions: { modelAssetPath: OBJECT_DETECTOR_MODEL },
        runningMode: "VIDEO",
        scoreThreshold: 0.5,
        maxResults: 10
      });
    }

    function fitCanvas(){
      const rect = els.stage.getBoundingClientRect();
      els.canvas.width = rect.width;
      els.canvas.height = rect.width * (els.video.videoHeight / Math.max(1, els.video.videoWidth));
      // Set video CSS transform for mirror
      els.video.style.transform = els.toggleMirror.checked ? "scaleX(-1)" : "none";
      els.canvas.style.transform = els.toggleMirror.checked ? "scaleX(-1)" : "none";
      els.res.textContent = `${els.video.videoWidth}×${els.video.videoHeight}`;
    }

    async function startCamera(){
      stopAll();
      try{
        stream = await navigator.mediaDevices.getUserMedia({
          video: {
            width: { ideal: 1280 },
            height: { ideal: 720 },
            facingMode: "user" // selfie camera on mobile
          },
          audio: false
        });
      }catch(err){
        alert("Camera access failed: " + err.message + "\n\nOn iOS, open via HTTPS and allow camera in Settings.");
        throw err;
      }
      els.video.srcObject = stream;
      await els.video.play();
      fitCanvas();
      window.addEventListener('resize', fitCanvas);
      running = true;
      loop();
    }

    function stopAll(){
      running = false;
      if (stream){
        stream.getTracks().forEach(t=>t.stop());
        stream = null;
      }
      window.removeEventListener('resize', fitCanvas);
      els.btnStart.disabled = false;
      els.btnStop.disabled = true;
    }

    function drawBox(ctx, x, y, w, h, label, score, style="face"){
      ctx.save();
      const pad = 2;
      ctx.lineWidth = 2;
      ctx.strokeStyle = style==="obj" ? "#88e" : "#5cb85c";
      ctx.fillStyle = "rgba(0,0,0,.45)";
      ctx.beginPath(); ctx.rect(x,y,w,h); ctx.stroke();

      const text = score!=null ? `${label} ${(score*100).toFixed(0)}%` : label;
      ctx.font = "13px system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial";
      const m = ctx.measureText(text);
      const boxW = m.width + 12, boxH = 20;
      const tx = Math.max(0, Math.min(x, ctx.canvas.width - boxW));
      const ty = Math.max(0, y - boxH - 6);
      ctx.fillRect(tx, ty, boxW, boxH);
      ctx.strokeStyle = "rgba(255,255,255,.2)";
      ctx.strokeRect(tx+.5, ty+.5, boxW-1, boxH-1);
      ctx.fillStyle = "#fff";
      ctx.fillText(text, tx + 6, ty + 14);
      ctx.restore();
    }

    // Map MediaPipe blendshapes to a simple emotion label
    function estimateEmotion(blendshapes){
      // blendshapes: array of {categories:[{categoryName, score}]}
      if (!blendshapes || !blendshapes.length) return {emotion:"neutral", conf:0.5};
      const cat = {};
      for (const c of blendshapes[0].categories){
        cat[c.categoryName] = c.score;
      }
      const smile = ((cat["mouthSmileLeft"]||0) + (cat["mouthSmileRight"]||0))/2;
      const jawOpen = (cat["jawOpen"]||0);
      const browUp = (cat["browInnerUp"]||0);
      const browDown = ((cat["browDownLeft"]||0)+(cat["browDownRight"]||0))/2;
      const eyeSquint = ((cat["eyeSquintLeft"]||0)+(cat["eyeSquintRight"]||0))/2;
      const mouthFrown = ((cat["mouthFrownLeft"]||0)+(cat["mouthFrownRight"]||0))/2;

      // Simple rules with soft confidence
      let emotion = "neutral";
      let conf = 0.5;
      if (smile > 0.55 && eyeSquint > 0.2) { emotion = "happy"; conf = lerp(0.6, 0.95, Math.min(1, (smile-0.55)/0.4)); }
      else if (jawOpen > 0.55 && browUp > 0.3) { emotion = "surprised"; conf = lerp(0.6, 0.95, Math.min(1, (jawOpen-0.55)/0.4)); }
      else if (browDown > 0.45 && mouthFrown > 0.25) { emotion = "angry"; conf = lerp(0.6, 0.9, Math.min(1, (browDown-0.45)/0.4)); }
      else if (mouthFrown > 0.35 && smile < 0.25) { emotion = "sad"; conf = lerp(0.6, 0.88, Math.min(1, (mouthFrown-0.35)/0.4)); }
      else { emotion = "neutral"; conf = 0.6 - 0.2*Math.random(); }
      return {emotion, conf};
    }

    async function loop(){
      if (!running) return;
      const now = performance.now();
      const dt = now - lastTime;
      lastTime = now;
      const instFps = 1000 / Math.max(1, dt);
      fpsSMA = fpsSMA ? lerp(fpsSMA, instFps, 0.15) : instFps;
      els.fps.textContent = Math.round(fpsSMA);

      const ctx = els.canvas.getContext('2d');
      const video = els.video;

      // Resize canvas if video size changes
      if (els.canvas.width !== video.clientWidth){
        fitCanvas();
      }
      // Draw video frame to canvas background (so we can mirror both consistently)
      ctx.clearRect(0,0,els.canvas.width, els.canvas.height);

      // Compute scaling from video pixel space to canvas space
      const vw = video.videoWidth, vh = video.videoHeight;
      const cw = els.canvas.width, ch = els.canvas.height;
      const scaleX = cw / vw, scaleY = ch / vh;

      let faces = null, objs = null;

      if (faceLandmarker && els.toggleFaces.checked){
        faces = faceLandmarker.detectForVideo(video, now);
      }
      if (objectDetector && els.toggleObjs.checked && (frameId % objEveryNFrames === 0)){
        objs = objectDetector.detectForVideo(video, now);
      }
      frameId++;

      detectionsCount = 0;

      // Draw Objects
      if (objs && objs.detections){
        for (const d of objs.detections){
          const b = d.boundingBox; // {originX, originY, width, height} in video px
          const x = b.originX * scaleX, y = b.originY * scaleY, w = b.width * scaleX, h = b.height * scaleY;
          const cat = d.categories && d.categories[0] ? d.categories[0] : null;
          const label = cat ? cat.categoryName : "object";
          const score = cat ? cat.score : null;
          drawBox(ctx, x, y, w, h, label, score, "obj");
          detectionsCount++;
        }
      }

      // Draw Faces + Expressions
      if (faces && faces.faceLandmarks && faces.faceBlendshapes){
        const draw = new DrawingUtils(ctx);
        faces.faceLandmarks.forEach((lm, idx) => {
          // Landmarks polyline (subtle)
          ctx.save();
          ctx.globalAlpha = 0.35;
          ctx.lineWidth = 1;
          ctx.strokeStyle = "#5cb85c";
          draw.drawConnectors(lm, FaceLandmarker.FACE_LANDMARKS_TESSELATION);
          ctx.restore();

          // Bounding box from landmarks
          let minX=1e9, minY=1e9, maxX=-1e9, maxY=-1e9;
          for (const p of lm){
            minX = Math.min(minX, p.x * cw);
            minY = Math.min(minY, p.y * ch);
            maxX = Math.max(maxX, p.x * cw);
            maxY = Math.max(maxY, p.y * ch);
          }
          const pad = 8;
          const x = Math.max(0, minX - pad);
          const y = Math.max(0, minY - pad);
          const w = Math.min(cw - x, (maxX - minX) + pad*2);
          const h = Math.min(ch - y, (maxY - minY) + pad*2);

          let label = "face";
          let conf = null;
          if (els.toggleExpr.checked){
            const {emotion, conf: econf} = estimateEmotion(faces.faceBlendshapes[idx]?.categories ? [faces.faceBlendshapes[idx]] : faces.faceBlendshapes);
            label = `${emotion}`;
            conf = econf;
          }
          drawBox(ctx, x, y, w, h, label, conf, "face");
          detectionsCount++;
        });
      }

      els.det.textContent = detectionsCount.toString();

      // schedule next frame
      requestAnimationFrame(loop);
    }

    // UI wiring
    els.btnStart.addEventListener('click', async ()=>{
      els.btnStart.disabled = true;
      try{
        if (!faceLandmarker || !objectDetector){
          await loadModels();
        }
        await startCamera();
        els.btnStop.disabled = false;
      }catch(e){
        els.btnStart.disabled = false;
        console.error(e);
      }
    });
    els.btnStop.addEventListener('click', stopAll);
    els.toggleMirror.addEventListener('change', fitCanvas);

    // Helpful: pause when tab is hidden to save battery
    document.addEventListener('visibilitychange', ()=>{
      if (document.hidden){ running = false; }
      else if (stream){ running = true; loop(); }
    });
  </script>
</body>
</html>
